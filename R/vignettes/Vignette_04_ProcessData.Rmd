---
title: "Vignette 04: Process Data"
author: "Grace Smith-Vidaurre"
date: "2023-12-27"
output: html_document
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = TRUE, eval = TRUE)

```

<h2>Vignette Overview and Learning Objectives</h2>

In this fourth vignette, you will write out spreadsheets of simulated detections of animal movements to your computer. You'll begin using this data in the ABISSMAL data processing and analysis workflow, including combining raw data across days and pre-processing the raw data. You will continue to use coding skills that you learned in the previous vignettes, and you will learn additional skills that include:

1. Indexing and filtering data frames
2. Checking data frame structure with base R and the tidyverse
3. Saving objects from R as physical files on your computer 
4. Reading files from your computer into R
5. Sourcing custom functions
6. Using custom functions in an analytical pipeline

<h2>Load packages and your working directory path</h2>

```{r message = FALSE, warning = FALSE}

rm(list = ls()) # Clean global environment

library(tidyverse) # Load the set of tidyverse packages

path <- "/home/gsvidaurre/Desktop/ABISSMAL_vignettes" # Initialize an object with the path that is your working directory

```

<h3>Create the simulated data</h3>

In the code below, you'll recreate the simulated RFID and beam breaker datasets that you learned to build in the previous vignette. Here the code from vignette 03 has been condensed into fewer chunks.
```{r}

# Create a vector of 4 RFID timestamps in HH:MM:SS format
rfid_ts <- c("10:00:00", "10:05:00", "11:00:00", "11:05:00")

# Add perching events to the RFID data
rfid_ts <- c(rfid_ts, "08:00:00", "08:00:01", "08:00:02", "08:00:03", "11:30:00", "11:30:01", "11:30:02", "11:30:03", "11:30:04", "11:30:05")

glimpse(rfid_ts)

```

```{r}

# For the beam breakers, simulate timestamps for an entrance, an exit, and then another entrance and exit
irbb_ts <- c("09:59:59", "10:05:01", "10:59:59", "11:05:01")

# Simulate some RFID detection failures for the beam breaker data
irbb_ts <- c(irbb_ts, "06:05:05", "06:35:08", "07:15:40", "11:10:25")

# Simulate some stray beam breaker detections
irbb_ts <- c(irbb_ts, "09:45:01", "09:45:02", "09:45:03", "09:45:04", "09:45:05", "09:45:06", "09:45:07", "09:45:08", "09:45:09", "09:45:10", "09:45:11")

glimpse(irbb_ts)
  
```

<h3>Simulate 2 days of RFID data collection</h3>

In the code below, you'll use combine the vector of RFID timestamps that you made above with metadata in a data frame. This metadata will include the year, month, and day, as well as a column with 2 unique PIT tag identifiers (1 for each simulated individual), and a column with the sensor type. You should recognize some of this code from the previous vignette:
```{r}

# Make a vector for the experimental replicate
exp_rep <- rep(x = "Pair_01", times = length(rfid_ts))

# Make the data frame with the experimental replicate metadata and the timestamps
sim_dats_rfid <- data.frame(replicate = exp_rep, timestamps = rfid_ts)

# Overwrite the data frame with the modified version that has columns for the year, month, and day
sim_dats_rfid <- sim_dats_rfid %>%
  dplyr::mutate(
    year = 2023
  ) %>%
  dplyr::mutate(
    month = 08,
    day = 01
  ) %>%
  # Allocate the first half of the detections to the first individual using nrow(.)/2, and the second half of the detctions to the second individual using nrow(.)/2 again. Both of the rep() expressions are combined into a single vector using the c() function
  dplyr::mutate(
    PIT_tag = c(rep("1357aabbcc", nrow(.)/2), rep("2468zzyyxx", nrow(.)/2))
  ) %>% 
  dplyr::mutate(
    sensor_id = "RFID"
  )

glimpse(sim_dats_rfid)

```

Next, you can use this data frame to simulate data collection over more than one day. To create more observations (rows) for two additional days, you can append rows from a modified copy of `sim_dats_rfid` to itself. 

In the code below, you're piping `sim_dats_rfid` into `bind_rows()`, which indicates that this is the original object to which you want to append new rows. Then the code inside of `bind_rows()` indicates the data frame (the new rows) that will be appended to `sim_dats_rfid`. In this case, the code inside of `bind_rows()` pipes `sim_dats_rfid` into `dplyr::mutate()`, which is a function that you're using to modify the `day` column to reflect a subsequent day of data collection.
```{r}

sim_dats_rfid <- sim_dats_rfid %>% 
  bind_rows(
    sim_dats_rfid %>% 
      dplyr::mutate(
        day = 02
      )
  )

glimpse(sim_dats_rfid) # Double the number of rows, looks good

```

<h3>Check data frame columns with base R</h3>

You checked that simulated data frame has data collected over two days looking at the structure of the new object. You can also check the unique values present in the column `day`. Below is a way of checking the unique values contained in a column of a data frame, using two different examples of base R notation for accessing columns:
```{r}

# The name of a data frame object, followed by a $ sign and then the name of a column allows you to pull out one column at a time from a data frame. A data frame column is a vector, so when you run this code you will see a vector of values print to the console
sim_dats_rfid$day

# You can also access a column in a data frame by using double square brackets after the name of the data frame, and placing the column name in quotes inside of the inner brackets
sim_dats_rfid[["day"]]

# You can use the function unique() to see the unique values of any vector, including a column of a data frame
unique(sim_dats_rfid$day) # Two days, looks good

unique(sim_dats_rfid[["day"]]) # Two days, looks good

```

<h3>Check data frame columns with the tidyverse</h3>

You can also check the unique values in a column using functions from the tidyverse. In the expression below, you're piping `sim_dats_rfid` into the function `pull()`, which lets you pull the column `day` out of the data frame as a vector, and then that vector is piped into the function `unique()` to check the unique values contained in the vector. The function `unique()` does not need an argument inside of the parentheses here because you're already piping the output that it needs directly into the function.
```{r}

# Two days, looks good
sim_dats_rfid %>% 
  pull(day) %>% 
  unique()

```

<h3>Simulate 2 days of beam breaker data collection</h3>

Next, repeat this process of creating a data frame with metadata for the infrared beam breaker dataset. Since the beam breakers do not collect unique individual identity information, you will add columns for the year, month, day, and sensor type. You will also simulate data collection for the beam breakers over the same two days as the RFID system.
```{r}

# Overwrite the vector exp_rep with a new vector the same length as irbb_ts
exp_rep <- rep(x = "Pair_01", times = length(irbb_ts))

sim_dats_irbb <- data.frame(replicate = exp_rep, timestamps = irbb_ts)

sim_dats_irbb <- sim_dats_irbb %>%
  dplyr::mutate(
    year = 2023,
    month = 08,
    day = 01,
    sensor_id = "Beam breakers"
  )

glimpse(sim_dats_irbb)

sim_dats_irbb <- sim_dats_irbb %>% 
  bind_rows(
    sim_dats_irbb %>% 
      dplyr::mutate(
        day = 02
      )
  )

glimpse(sim_dats_irbb) # Double the number of rows, looks good

# Two days, looks good
sim_dats_irbb %>% 
  pull(day) %>% 
  unique()

```

<h2>Save data frames as physical files</h2>

Now that you made data frames of the timestamps and metadata per sensor, you can write out or save these data frames as physical files in your working directory. You have many different file type options, but I recommend using .csv format for data frames since this file type is compatible with R as well as Microsoft Word and other programs like LibreOffice.

In the code below, you'll write out a spreadsheet per sensor for each day of data collection. You're writing out a spreadsheet per day in order to use the first ABISSMAL function, which combines data collected across days per sensor into a single spreadsheet and in the right format for downstream functions.

In order to write out a spreadsheet per sensor and day, you need to filter the rows of the full data frame for each sensor by date. To do this filtering, you'll use a conditional statement inside of the function `dplyr::filter()`:
```{r}

# Pipe the data frame into the filter() function
sim_dats_rfid %>% 
  # Filter the data frame by pulling out all rows in which the day column was equal to 1
  dplyr::filter(day == 1) %>%
  glimpse()

# One day, looks good
sim_dats_rfid %>% 
  dplyr::filter(day == 1) %>%
  pull(day) %>% 
  unique()

```

You can now use this `dplyr::filter()` function as you write out a spreadsheet per day. In the code below, you will create a custom file name for the 1st day of RFID data, then you'll filter the RFID data frame by the first day and write it out as a .csv file:
```{r eval = FALSE}

?write.csv

# For the RFID data, create a vector of the working directory and the file name that you will use for the argument file in write.csv()
# The function file.path() will combine both pieces of information into a single file path
rfid_file <- file.path(path, "RFID_simulated_Pair-01_2023_08_01.csv")
rfid_file

sim_dats_rfid %>% 
  dplyr::filter(day == 1) %>% 
  # Write out the data frame as a .csv spreadsheet. Do not include row names
  # Remember that the "." means the function will use the object that is piped in, which here is the data frame filtered by day 1
  write.csv(x = ., file = rfid_file, row.names = FALSE)

```

As specified in the function documentation for `write.csv()`, the function will include column names in the resulting spreadsheet by default. The function will also not append new information to the .csv if it already exists. If you already created this file then it will be overwritten when you run the function again.

Repeat this process for the second day of RFID data:
```{r}

rfid_file <- file.path(path, "RFID_simulated_Pair-01_2023_08_02.csv")
rfid_file

sim_dats_rfid %>% 
  dplyr::filter(day == 2) %>% 
  write.csv(x = ., file = rfid_file, row.names = FALSE)

```

You can also write out a spreadsheet per day for the infrared beam breaker dataset.
```{r eval = FALSE}

irbb_file <- file.path(path, "IRBB_simulated_Pair-01_2023_08_01.csv")
irbb_file

sim_dats_irbb %>% 
  dplyr::filter(day == 1) %>% 
  # By default, write.csv() will write out the object that is piped in, so you don't need to specify "x = ."
  write.csv(irbb_file, row.names = FALSE)

irbb_file <- file.path(path, "IRBB_simulated_Pair-01_2023_08_02.csv")
irbb_file

sim_dats_irbb %>% 
  dplyr::filter(day == 2) %>% 
  # By default, write.csv() will write out the object that is piped in, so you don't need to specify "x = ."
  write.csv(irbb_file, row.names = FALSE)

```

Check that these 2 files now exist in your working directory.
```{r eval = FALSE}

list.files(path)

```

You can read one these files back into R with the function `read.csv()`. In the code below, you're piping the output of `read.csv()` directly into `glimpse()` to check out the structure of the resulting data frame. The output of the code is printed to the console but is not saved inside of an object for later manipulation. Instead, you will be using a custom function from ABISSMAL that automatically reads in each spreadsheet.
```{r}

read.csv(file.path(path, "IRBB_simulated_Pair-01_2023_08_02.csv")) %>% 
  glimpse()

```

<h2>Load ABISSMAL functions</h2>

The custom R functions available through the ABISSMAL GitHub repository are stored in physical files (extension .R) inside the local repository on your computer (which you should have downloaded in vignette 01). In order to start using the ABISSMAL functions, you need to load the physical .R files so that the functions are available in your global environment. In the code below, you will use the function `source()` to load 3 of the 5 main ABISSMAL functions, plus a script that holds a set of utility functions:
```{r}

# Load the function that combines raw data
source("/home/gsvidaurre/Desktop/GitHub_repos/ABISSMAL/R/combine_raw_data.R")

# Load the function that detects perching events in the raw data
source("/home/gsvidaurre/Desktop/GitHub_repos/ABISSMAL/R/detect_perching_events.R")

# Load the function that pre-processes raw data
source("/home/gsvidaurre/Desktop/GitHub_repos/ABISSMAL/R/preprocess_detections.R")

# Load a script with utility functions that each function above requires
source("/home/gsvidaurre/Desktop/GitHub_repos/ABISSMAL/R/utilities.R")

```

<h2>Access ABISSMAL function information</h2>

After running the lines of code above, you should see that a whole set of functions have been loaded into your global environment (check the `Environment` pane). Many of these functions start with `check_`, and those are utility functions. If you scroll down, you'll see that the three main functions above (`combine_raw_data`, `detect_perching_events`, `preprocess_detections`) are all loaded in your global environment. In the column to the right of the function names you can also see a preview of each function's arguments. 

To get more information about each of these three main functions, you can click the white square icon to the very right of each function in the `Environment` pane, or run the code `View(function_name)`. This will open the script for the given function a new tab in your Source pane. In each script for each function, you'll see lines of documentation starting with the symbols "`# @". You'll see the function name and description first, and then a description of each argument (parameter) for the function. If you keep scrolling down, you'll see a section with details about how the given function works, and the information that it returns. After the lines of documentation, you'll see the code that makes up the function itself. 

<h2>Combine raw data</h2>

Once you've loaded the ABISSMAL functions, you can start using the first function, `combine_raw_data()`, to combine data collected across days per sensor into a single spreadsheet per sensor.
```{r}

combine_raw_data()

```

